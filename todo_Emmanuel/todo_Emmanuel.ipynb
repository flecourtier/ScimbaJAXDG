{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa7942d8",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e365f3e1",
   "metadata": {},
   "source": [
    "## Idée générale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab5e72d",
   "metadata": {},
   "source": [
    "On écrit $u_\\theta$ sous la forme :\n",
    "\n",
    "$$u_\\theta(x) = \\sum_{i=1}^N \\alpha_i \\varphi_i(x,\\beta_i)$$\n",
    "\n",
    "avec $\\theta = (\\alpha, \\beta)$ et où chaque $\\varphi_i$ est un réseau de neurones avec des paramètres $\\beta_i$.\n",
    "\n",
    "Pour ne pas avoir à optimiser les $\\alpha_i$ et les $\\beta_i$ simultanément ou de manière alternée, on peut utiliser la structure linéaire du problème pour résoudre exactement les $\\alpha_i$ en fonction des $\\beta_i$. Plus précisément, pour un ensemble donné de paramètres $\\beta$, on peut construire la matrice $A(\\beta)$ et le vecteur $b$ tels que :\n",
    "\n",
    "$$A(\\beta) \\vec\\alpha = b$$\n",
    "\n",
    "Ainsi l'inconnu $u_\\theta$ devient :\n",
    "$$u_\\beta(x) = \\sum_{i=1}^N \\left(A(\\beta)^{-1} b\\right)_i \\varphi_i(x,\\beta_i)$$\n",
    "\n",
    "et le problème de minimisation devient un problème d'optimisation uniquement sur les paramètres $\\beta$ :\n",
    "$$\\beta^* = \\arg\\min_\\beta \\|\\Delta u_\\beta + f\\|_{L^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f44424d",
   "metadata": {},
   "source": [
    "## Devoirs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc99fd",
   "metadata": {},
   "source": [
    "### Premier point &#x2611; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb33275",
   "metadata": {},
   "source": [
    "**Objectif :** Implémenter la résolution de problèmes linéaires et non linéaires en 1D à l'aide de schémas de différences finies en utilisant JAX pour les calculs différentiables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf26acf",
   "metadata": {},
   "source": [
    "On considére le problème suivant en 1D:\n",
    "\n",
    "$$\\left\\{\\begin{align*}\n",
    "-u''(x)+g(u) &= f(x),\\quad x\\in(0,1) \\\\\n",
    "u(0) = u(1) &= 0\\end{align*}\\right.$$\n",
    "\n",
    "pour $g=\\alpha u$ (linéaire) et pour $g=\\alpha u^2$ (non linéaire).\n",
    "\n",
    "On applique un schéma différences finies centré d'ordre 2 sur un maillage uniforme : \n",
    "\n",
    "$$\\left\\{\\begin{align*}\n",
    "-u_{i+1}+2u_i-u_{i-1} + \\alpha h^2 u_i &= h^2f_i,\\quad i=1,\\ldots,N-1 \\\\\n",
    "u_0 = u_N &= 0\\end{align*}\\right.$$ \n",
    "\n",
    "- Dans le cas où le système est **linéaire**, on peut donc le résoudre **directement** :\n",
    "\n",
    "    $$A\\mathbf{u} = \\mathbf{f}$$\n",
    "\n",
    "    avec $A$, $\\mathbf{u}$ et $\\mathbf{f}$ définis par :\n",
    "\n",
    "    $$A = \\frac{1}{h^2}\\begin{pmatrix}\n",
    "    2+\\alpha h^2 & -1 & 0 & \\cdots & 0 \\\\\n",
    "    -1 & 2+\\alpha h^2 & -1 & \\cdots & 0 \\\\\n",
    "    0 & -1 & 2+\\alpha h^2 & \\cdots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\cdots & 2+\\alpha h^2\n",
    "    \\end{pmatrix}, \\quad \\mathbf{u} = \\begin{pmatrix}\n",
    "    u_1 \\\\ u_2 \\\\ u_3 \\\\ \\vdots \\\\ u_{N-1}\n",
    "    \\end{pmatrix}, \\quad \\mathbf{f} = \\begin{pmatrix}\n",
    "    f_1 \\\\ f_2 \\\\ f_3 \\\\ \\vdots \\\\ f_{N-1}\n",
    "    \\end{pmatrix}$$\n",
    "\n",
    "- Dans le cas où le système est **non-linéaire**, on utilise donc la **méthode de Newton-Raphson** pour le résoudre :\n",
    "\n",
    "    $$\\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} - J_F(\\mathbf{u}^{(k)})^{-1} F(\\mathbf{u}^{(k)})$$\n",
    "\n",
    "    où $F(\\mathbf{u})$ est le résidu et $J_F$ sa Jacobienne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e5117",
   "metadata": {},
   "source": [
    "### Second point  &#x2611; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cdd4a0",
   "metadata": {},
   "source": [
    "**Objectif :** Etudier la dépendance de la solution par rapport à $\\alpha$ en utilisant la différentiation automatique de JAX pour calculer les dérivées de la solution par rapport à $\\alpha$. Appliquer le théorème des fonctions implicites pour comprendre comment la solution change lorsque $\\alpha$ varie, et vérifier les résultats obtenus par différentiation automatique avec des calculs analytiques ou numériques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5d440",
   "metadata": {},
   "source": [
    "### Troisième point &#x2612;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d736a7b",
   "metadata": {},
   "source": [
    "**Objectif :** Même objectif mais avec du Matrix-Free. \n",
    "\n",
    "- *V1 :* Tester avec la librairie `jaxopt` ([Nonlinear least squares](https://jaxopt.github.io/stable/nonlinear_least_squares.html)).\n",
    "\n",
    "- *V2 :* Implémenter la méthode de Newton à la main.\n",
    "\n",
    "    - Forward (Méthode de Newton)\n",
    "\n",
    "        **Initialisation :** $u_0$\n",
    "\n",
    "        **Pour $i = 1, \\dots$ :**\n",
    "\n",
    "        $$ J(u_k) \\delta u = -F(u_k) $$\n",
    "\n",
    "        $$ u_{k+1} = \\dots $$\n",
    "\n",
    "        > **Note :** On résout ce système avec un **Gradient conjugué** (À écrire) qui prend l'opérateur **JVP** de $F(u)$. $\\longrightarrow$ *par rapport à $u$*\n",
    "\n",
    "    - Backward (Méthode de l'Adjoint)\n",
    "\n",
    "        Prend en entrée $u^*$ (la solution convergée) et une direction $v$.\n",
    "\n",
    "        $$ J(u^*)^T r = v $$\n",
    "\n",
    "        > **Note :** Résolu via **Gradient conjugué** en utilisant le **VJP** de $F(u^*)$. $\\longrightarrow$ *par rapport à $u$*\n",
    "\n",
    "    - Sensibilité / Gradient\n",
    "\n",
    "        $$ \\frac{\\partial u^*}{\\partial \\alpha} = \\left( \\frac{\\partial F}{\\partial \\alpha} \\right)^T r $$\n",
    "\n",
    "        > **Note :** Correspond au **VJP de $F$**  $\\longrightarrow$ *par rapport à $\\alpha$*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
